<configuration>
<property>
 <name>dfs.replication</name>
 <value>3</value>
</property>
<property>
 <name>dfs.namenode.name.dir</name>
 <value>/opt/hadoop/hdfs/namenode</value>
</property>
<property>
 <name>dfs.datanode.data.dir</name>
 <value>/opt/hadoop/hdfs/datanode</value>
 <description>Determines where on the local filesystem an DFS data node should store its blocks. If this is a comma-delimited list of directories, then data will be stored in all n
amed directories, typically on different devices. Directories that do not exist are ignored.
 </description>
</property>
<!-- Not sure if it works -->
<!--<property>
        <name>dfs.datanode.use.datanode.hostname</name>
        <value>false</value>
    </property>
   
    <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
    </property> 
    -->
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>ZKNODES</value>
    </property>
    <property>
      <name>dfs.nameservices</name>
      <value>CLUSTER_NAME</value>
    </property>
    <property>
      <name>dfs.ha.namenodes.CLUSTER_NAME</name>
      <value>nn1,nn2</value>
    </property>
    <property>
      <name>dfs.namenode.rpc-address.CLUSTER_NAME.nn1</name>
      <value>NNODE1_IP:8020</value>
    </property>
    <property>
      <name>dfs.namenode.rpc-address.CLUSTER_NAME.nn2</name>
      <value>NNODE2_IP:8020</value>
    </property>
    <property>
      <name>dfs.namenode.http-address.CLUSTER_NAME.nn1</name>
      <value>NNODE1_IP:50070</value>
    </property>
    <property>
      <name>dfs.namenode.http-address.CLUSTER_NAME.nn2</name>
      <value>NNODE2_IP:50070</value>
    </property>
    <!--<property>
      <name>dfs.namenode.shared.edits.dir</name>
      <value>qjournal://JNODES/CLUSTER_NAME</value>
    </property>-->
    <property>
      <name>dfs.client.failover.proxy.provider.mycluster</name>
      <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
      <name>dfs.ha.fencing.methods</name>
      <value>shell(/etc/fence.sh)</value>
    </property>
   <!-- <property>
      <name>dfs.journalnode.edits.dir</name>
      <value>/mnt/hadoop/journal/data</value>
    </property> -->
    </configuration>
